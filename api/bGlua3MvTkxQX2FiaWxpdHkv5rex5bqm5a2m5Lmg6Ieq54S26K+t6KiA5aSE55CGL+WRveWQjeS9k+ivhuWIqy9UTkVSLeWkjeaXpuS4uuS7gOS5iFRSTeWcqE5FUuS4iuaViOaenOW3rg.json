{"title":"","date":"2025-12-02T22:37:32.940Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:37:32.940Z","content":"<p>今天介绍复旦的一个论文<a href=\"https://arxiv.org/pdf/1911.04474.pdf,\" title=\"TENER: Adapting Transformer Encoder for Named Entity Recognition\" target=\"_blank\">TENER</a> ；普通的TRM在其他NLP任务中效果很不错，但是在NER中表现不佳。为了解决性能不佳，论文做了几点改进。</p>\n<p>主要掌握以下三点改进：</p>\n<ol>\n<li>方向</li>\n<li>距离</li>\n<li>无缩放的注意力</li>\n</ol>\n<h1 id=\"1.-架构图\">1. 架构图<a title=\"#1.-架构图\" href=\"#1.-架构图\"></a></h1>\n<p>先看TENER架构图：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-09-040433.jpg\" alt=\"TENER架构图\" loading=\"lazy\" class=\"φbp\"></p>\n<h1 id=\"2.-距离和方向信息\">2. 距离和方向信息<a title=\"#2.-距离和方向信息\" href=\"#2.-距离和方向信息\"></a></h1>\n<p>对于NER任务来说，距离和方向都很重要；</p>\n<p>举个简单的例子：【李华住在北京】；李华是人名，北京是地名，如果忽视了方向，那么【北京住在李华】，这个肯定是说不通的。</p>\n<p>换句话说，每类NER实体在哪种位置是有着某种关系或者规则的。所以方向很重要。</p>\n<p>简单概述普通TRM位置编码的问题，如下：</p>\n<p>普通TRM中的正弦位置编码能够捕捉到距离信息，但是不能捕捉到方向信息。而且这种基本性质（distance-awareness）会在sefl-attention消失；</p>\n<p>为了改进这种问题，使用了经过改进的相对位置编码，弃用了绝对位置编码；</p>\n<p><strong>2.1 为什么没有方向信息</strong>：</p>\n<p>位置编码的点积可以看做在度量两者之间的距离:$PE^{T}<em>{t}PE</em>{t+k}$</p>\n<p>点积结果画图表示如下：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-09-114414.png\" alt=\"点积\" loading=\"lazy\" class=\"φbp\"></p>\n<p>从这个图，我们可以很清楚的看到，是对称的，也就是说在k=20和k=-20的时候，点击结果相同，换句话说，方向信息没有体现出来。</p>\n<p>公式上体现就是：$PE^{T}<em>{t}PE</em>{t+k}=PE^{T}<em>{t-k}PE</em>{t}$</p>\n<p><strong>2.2 distance-awareness 消失</strong></p>\n<p>再进一步，在self-attention中，distance-awareness 也在消失，这一点，我之前的文章有写，可以看<a href=\"https://mp.weixin.qq.com/s?__biz=MzIyNTY1MDUwNQ==&amp;mid=2247483760&amp;idx=1&amp;sn=c2803e63bdd42e4d1f1f880ce9eda8cc&amp;chksm=e87d3356df0aba40c77356418647856ec135c731fd60122378ed702e1e959c820250c2293e1f&amp;token=588814416&amp;lang=zh_CN#rd\" target=\"_blank\">原版Transformer的位置编码究竟有没有包含相对位置信息</a>。</p>\n<p>改进之后的相对位置编码以及attention计算为：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-09-040434.jpg\" alt=\"新的attention和相对位置编码\" loading=\"lazy\" class=\"φbp\"></p>\n<h1 id=\"3.-attention缩放\">3. attention缩放<a title=\"#3.-attention缩放\" href=\"#3.-attention缩放\"></a></h1>\n<p>传统TRM的attention分布被缩放了，从而变得平滑。但是对于NER来说，一个更加尖锐或者说稀疏的矩阵是更合适的，因为并不是所有的单词都需要被关注；一个当前的单词的类别，足够被周围几个单词确定出来。</p>\n<p>矩阵越平滑，关注的单词越多，可能会引入更多的噪声信息。</p>\n<h1 id=\"4.-总结\">4. 总结<a title=\"#4.-总结\" href=\"#4.-总结\"></a></h1>\n<ol>\n<li>原始TRM绝对位置编码不含有方向信息，Self-attention之后相对位置信息也会消失；故使用改进的相对位置编码和新的attention计算方式</li>\n<li>attention计算不使用缩放系数，减少了噪声信息</li>\n<li>使用TRM进行char编码，结合预训练的词向量拼接输入TENER</li>\n</ol>\n","link":"links/NLP_ability/深度学习自然语言处理/命名体识别/TNER-复旦为什么TRM在NER上效果差","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/命名体识别/TNER-复旦为什么TRM在NER上效果差/","toc":[{"id":"1.-架构图","title":"1. 架构图","index":"1"},{"id":"2.-距离和方向信息","title":"2. 距离和方向信息","index":"2"},{"id":"3.-attention缩放","title":"3. attention缩放","index":"3"},{"id":"4.-总结","title":"4. 总结","index":"4"}],"reward":true}