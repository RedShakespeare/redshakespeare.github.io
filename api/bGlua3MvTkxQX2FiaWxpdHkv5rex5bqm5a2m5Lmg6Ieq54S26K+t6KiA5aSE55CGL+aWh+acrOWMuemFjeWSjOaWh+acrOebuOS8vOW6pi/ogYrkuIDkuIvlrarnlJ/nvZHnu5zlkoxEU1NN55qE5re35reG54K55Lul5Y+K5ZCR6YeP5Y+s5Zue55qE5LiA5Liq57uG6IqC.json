{"title":"","date":"2025-12-02T22:25:48.634Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:25:48.634Z","content":"<p>在学习DSSM的时候，很容易和孪生网络搞混，我这里稍微总结一下自己的思考；</p>\n<p>对于孪生网络，并不是一个网络的名称，而是一种网络的名称；</p>\n<p>它的特点就是encoder参数共享，也就是在对句子或者图像编码的网络权重共享；</p>\n<p>它的网络的输入形式是这样的:：(X1,X2,Y)；X1，X2是两个输入文本，Y是我们的标签数据；</p>\n<p>对于孪生网络来说，一般的损失函数是对比损失函数：Contrastive Loss</p>\n<p>什么是对比损失函数呢？公式如下：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2021-05-19-065215.png\" alt=\"image-20210114154303377\" loading=\"lazy\" class=\"φbp\"></p>\n<p>这个公式需要注意两个细节点，一个是d，代表的是距离度量，一个是margin，代表的是一个超参；</p>\n<p>那么我感兴趣的是孪生网路可以不可以使用其他的损失函数？如果可以，又是哪些呢？</p>\n<p>首先当然是可以，只要能够优化都可以；</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2021-05-19-065221.png\" alt=\"image-20210114154713043\" loading=\"lazy\" class=\"φbp\"></p>\n<p>参考自这里：</p>\n<p>Siamese network 孪生神经网络–一个简单神奇的结构 - mountain blue的文章 - 知乎 <a href=\"https://zhuanlan.zhihu.com/p/35040994\" target=\"_blank\">https://zhuanlan.zhihu.com/p/35040994</a></p>\n<h1 id=\"三种损失函数形式\">三种损失函数形式<a title=\"#三种损失函数形式\" href=\"#三种损失函数形式\"></a></h1>\n<p>我把自己思考的三种损失函数形式总结在这里，之后有问题再回来修改:</p>\n<p>两个向量做consine相似度或者欧氏距离度量函数，然后归一化到0-1，然后做二分类交叉熵损失函数；</p>\n<p>两个向量做cosine相似度或者欧氏距离，带入到对应的对比损失函数</p>\n<p>两个向量做拼接或者其他操作，然后接单个或者多个全连接，然后可以做逻辑回归或者做softmax；</p>\n<h1 id=\"fassi做向量召回-样本重复和consine度量疑惑点\">fassi做向量召回-样本重复和consine度量疑惑点<a title=\"#fassi做向量召回-样本重复和consine度量疑惑点\" href=\"#fassi做向量召回-样本重复和consine度量疑惑点\"></a></h1>\n<p>我其实一直有一个疑问，在做向量召回的时候，一般的操作就是双塔模型，然后存储对应的样本的向量，存储到fassi中，然后搜索的时候使用找最近的向量就够了；</p>\n<p>这里面我最开始理解的时候有两个疑惑点，一个是如果做到同一个样本不会有多个向量；</p>\n<p>我的误解原因是没有对向量的是如何落地有很好的理解；我开始的理解是模型训练好了之后，进入一个pair（query，d），分别计算向量，然后存储，这样当然会出现同一个d，出现在不同的pair；</p>\n<p>但是，由于之间没有交互，右边这个塔模型参数不变，得到的向量当然也不变，也就是同一个d不会出现多个向量；</p>\n<p>而且在计算的时候，不用输入一个pair对，只需要对右边这个单塔输入去重之后的d就可以；</p>\n<p>第二个问题，为什么在faiss中使用最近的向量可以（先不用计较度量方式）得到相似的向量，而在模型中，我们在得到cosine或者其他度量结果之后，还会再接一个sigmoid；</p>\n<p>优化sigmoid的输出的时候，cosine的值或者其他度量的值也会同等趋势变化，所以可以起到作用；</p>\n<p>然后多说一下，如果用到向量召回，中间还是需要一个度量的值的，需要和faiss对应上，如果接一个MLP，感觉就够呛。</p>\n","link":"links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/聊一下孪生网络和DSSM的混淆点以及向量召回的一个细节","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/聊一下孪生网络和DSSM的混淆点以及向量召回的一个细节/","toc":[{"id":"三种损失函数形式","title":"三种损失函数形式","index":"1"},{"id":"fassi做向量召回-样本重复和consine度量疑惑点","title":"fassi做向量召回-样本重复和consine度量疑惑点","index":"2"}],"reward":true}