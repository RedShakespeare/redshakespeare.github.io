{"title":"","date":"2025-12-02T22:25:48.564Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:25:48.564Z","content":"<h1 id=\"背景介绍\">背景介绍<a title=\"#背景介绍\" href=\"#背景介绍\"></a></h1>\n<p>NLP日常工作经验和论文解析，包含：预训练模型，文本表征，文本相似度，文本分类，多模态，知识蒸馏，词向量。</p>\n<p>我觉得NLP是一个值得深耕的领域，所以希望可以不停的提升自己核心竞争力和自己的段位！</p>\n<p>微信公众号：DASOU</p>\n<h2 id=\"深度学习自然语言处理\">深度学习自然语言处理<a title=\"#深度学习自然语言处理\" href=\"#深度学习自然语言处理\"></a></h2>\n<h3 id=\"transformer\">Transformer<a title=\"#transformer\" href=\"#transformer\"></a></h3>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8Transformer%E9%9D%A2%E8%AF%95%E9%A2%98\">史上最全Transformer面试题</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E7%AD%94%E6%A1%88%E8%A7%A3%E6%9E%90%E2%80%94%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8Transformer%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E7%81%B5%E9%AD%8220%E9%97%AE%E5%B8%AE%E4%BD%A0%E5%BD%BB%E5%BA%95%E6%90%9E%E5%AE%9ATransformer\">答案解析(1)-史上最全Transformer面试题</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/Pytorch%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90-%E5%A6%82%E4%BD%95%E8%AE%A9Bert%E5%9C%A8finetune%E5%B0%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%97%B6%E6%9B%B4%E2%80%9C%E7%A8%B3%E2%80%9D%E4%B8%80%E7%82%B9\">Pytorch代码分析–如何让Bert在finetune小数据集时更“稳”一点</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/%E8%A7%A3%E5%86%B3%E8%80%81%E5%A4%A7%E9%9A%BE%E9%97%AE%E9%A2%98-%E5%A6%82%E4%BD%95%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%B8%A6%E4%BD%A0%E9%9A%8F%E5%BF%83%E6%89%80%E6%AC%B2%E9%87%8D%E6%96%B0%E5%88%9D%E5%A7%8B%E5%8C%96bert%E7%9A%84%E6%9F%90%E4%BA%9B%E5%8F%82%E6%95%B0%E9%99%84Pytorch%E4%BB%A3%E7%A0%81\">解决老大难问题-如何一行代码带你随心所欲重新初始化bert的某些参数(附Pytorch代码详细解读)</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/3%E5%88%86%E9%92%9F%E4%BB%8E%E9%9B%B6%E8%A7%A3%E8%AF%BBTransformer%E7%9A%84Encoder\">3分钟从零解读Transformer的Encoder</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E5%8E%9F%E7%89%88Transformer%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%A9%B6%E7%AB%9F%E6%9C%89%E6%B2%A1%E6%9C%89%E5%8C%85%E5%90%AB%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF\">原版Transformer的位置编码究竟有没有包含相对位置信息</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/BN%E8%B8%A9%E5%9D%91%E8%AE%B0--%E8%B0%88%E4%B8%80%E4%B8%8BBatch%20Normalization%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E5%92%8C%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF\">BN踩坑记–谈一下Batch Normalization的优缺点和适用场景</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E8%B0%88%E4%B8%80%E4%B8%8B%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81\">谈一下相对位置编码</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/NLP%E4%BB%BB%E5%8A%A1%E4%B8%AD-layer-norm%E6%AF%94BatchNorm%E5%A5%BD%E5%9C%A8%E5%93%AA%E9%87%8C\">NLP任务中-layer-norm比BatchNorm好在哪里</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E8%B0%88%E4%B8%80%E8%B0%88Decoder%E6%A8%A1%E5%9D%97\">谈一谈Decoder模块</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/Transformer%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%8C%96\">Transformer的并行化</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E7%AD%94%E6%A1%88%E5%90%88%E8%BE%91\">Transformer全部文章合辑</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%85%B6%E4%BB%96/RNN%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8E%E4%BC%97%E4%B8%8D%E5%90%8C%E7%9A%84%E5%9C%B0%E6%96%B9\">RNN的梯度消失有什么与众不同的地方</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/VIT-%E5%A6%82%E4%BD%95%E5%B0%86Transformer%E6%9B%B4%E5%A5%BD%E7%9A%84%E5%BA%94%E7%94%A8%E5%88%B0CV%E9%A2%86%E5%9F%9F\">VIT-如何将Transformer更好的应用到CV领域</a></li>\n</ol>\n<h3 id=\"bert-基本知识\">Bert-基本知识<a title=\"#bert-基本知识\" href=\"#bert-基本知识\"></a></h3>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/FastBert\">FastBERT-CPU推理加速10倍</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/RoBERTa\">RoBERTa：更多更大更强</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/%E4%B8%BA%E4%BB%80%E4%B9%88Bert%E5%81%9A%E4%B8%8D%E5%A5%BD%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D\">为什么Bert做不好无监督语义匹配</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/UniLM\">UniLM:为Bert插上文本生成的翅膀</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/tBERT-BERT%E8%9E%8D%E5%90%88%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B\">tBERT-BERT融合主题模型做文本匹配</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/XLNET\">XLNET模型从零解读</a>.<br>\n<a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/BERT/%E5%A6%82%E4%BD%95%E5%9C%A8%E8%84%B1%E6%95%8F%E6%95%B0%E6%8D%AE%E4%B8%AD%E4%BD%BF%E7%94%A8BERT%E7%AD%89%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B\">如何在脱敏数据中使用BERT等预训练模型</a></li>\n</ol>\n<h3 id=\"bert-知识蒸馏\">Bert-知识蒸馏<a title=\"#bert-知识蒸馏\" href=\"#bert-知识蒸馏\"></a></h3>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/%E4%BB%80%E4%B9%88%E6%98%AF%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F\">什么是知识蒸馏</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/bert2textcnn%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F\">如何让 TextCNN 逼近 Bert</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/Bert%E8%92%B8%E9%A6%8F%E5%88%B0%E7%AE%80%E5%8D%95%E7%BD%91%E7%BB%9Clstm\">Bert蒸馏到简单网络lstm</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/PKD-Bert%E5%9F%BA%E4%BA%8E%E5%A4%9A%E5%B1%82%E7%9A%84%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%96%B9%E5%BC%8F\">PKD-Bert基于多层的知识蒸馏方式</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/Theseus-%E6%A8%A1%E5%9D%97%E5%8E%8B%E7%BC%A9%E4%BA%A4%E6%9B%BF%E8%AE%AD%E7%BB%83\">BERT-of-Theseus-模块压缩交替训练</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/tinybert-%E5%85%A8%E6%96%B9%E4%BD%8D%E8%92%B8%E9%A6%8F\">tinybert-全方位蒸馏</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/ALBERT-%E6%9B%B4%E5%B0%8F%E6%9B%B4%E5%B0%91%E4%BD%86%E5%B9%B6%E4%B8%8D%E5%BF%AB\">ALBERT：更小更少但并不快</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/BERT%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%A6%82%E4%BD%95%E5%86%99%E5%A5%BD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\">BERT知识蒸馏代码解析-如何写好损失函数</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E7%BB%BC%E8%BF%B0%E4%B8%87%E5%AD%97%E9%95%BF%E6%96%87\">知识蒸馏综述万字长文</a></li>\n</ol>\n<h3 id=\"词向量-word-embedding\">词向量-word embedding<a title=\"#词向量-word-embedding\" href=\"#词向量-word-embedding\"></a></h3>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8%E8%AF%8D%E5%90%91%E9%87%8F%E9%9D%A2%E8%AF%95%E9%A2%98%E6%A2%B3%E7%90%86\">史上最全词向量面试题-Word2vec/fasttext/glove/Elmo</a></li>\n</ol>\n<ul>\n<li>Word2vec</li>\n</ul>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/%E8%81%8A%E4%B8%80%E4%B8%8BWord2vec-%E6%A8%A1%E5%9E%8B%E7%AF%87\">Word2vec两种训练模型详细解读-一个词经过模型训练可以获得几个词向量</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/%E8%81%8A%E4%B8%80%E4%B8%8BWord2vec-%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96%E7%AF%87\">Word2vec两种优化方式细节详细解读</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/word2vec%E4%B8%A4%E7%A7%8D%E4%BC%98%E5%8C%96%E6%96%B9%E5%BC%8F%E7%9A%84%E8%81%94%E7%B3%BB%E5%92%8C%E5%8C%BA%E5%88%AB\">Word2vec-负采样和层序softmax与原模型是否等价</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/Word2vec%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E4%BA%8C%E6%AC%A1%E9%87%87%E6%A0%B7%EF%BC%9F\">Word2vec为何需要二次采样以及相关细节详细解读</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/Word2vec%E7%9A%84%E8%B4%9F%E9%87%87%E6%A0%B7\">Word2vec的负采样</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/Word2vec%E6%A8%A1%E5%9E%8B%E7%A9%B6%E7%AB%9F%E6%98%AF%E5%A6%82%E4%BD%95%E8%8E%B7%E5%BE%97%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84\">Word2vec模型究竟是如何获得词向量的</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/Word2vec%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E7%9A%84%E9%80%89%E5%AE%9A\">Word2vec训练参数的选定</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/CBOW%E5%92%8Cskip-gram%E7%9B%B8%E8%BE%83%E8%80%8C%E8%A8%80%EF%BC%8C%E5%BD%BC%E6%AD%A4%E7%9B%B8%E5%AF%B9%E9%80%82%E5%90%88%E5%93%AA%E4%BA%9B%E5%9C%BA%E6%99%AF\">CBOW和skip-gram相较而言，彼此相对适合哪些场景</a></li>\n</ol>\n<ul>\n<li>Fasttext/Glove</li>\n</ul>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/Fasttext%E8%A7%A3%E8%AF%BB1\">Fasttext详解解读(1)-文本分类</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/Fasttext%E8%A7%A3%E8%AF%BB2\">Fasttext详解解读(2)-训练词向量</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%91%E9%87%8F/%E8%81%8A%E4%B8%80%E4%B8%8BGlove\">GLove细节详细解读</a></li>\n</ol>\n<h3 id=\"多模态\">多模态<a title=\"#多模态\" href=\"#多模态\"></a></h3>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%A4%9A%E6%A8%A1%E6%80%81/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%B9%8BViLBERT%EF%BC%9A%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C%EF%BC%8C%E5%90%84%E8%87%AA%E4%B8%BA%E7%8E%8B\">多模态之ViLBERT：双流网络，各自为王</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%A4%9A%E6%A8%A1%E6%80%81/%E5%A4%8D%E7%9B%98%E5%A4%9A%E6%A8%A1%E6%80%81%E9%9C%80%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%846%E4%B8%AA%E9%97%AE%E9%A2%98\">复盘多模态任务落地的六大问题</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%A4%9A%E6%A8%A1%E6%80%81/%E5%A6%82%E4%BD%95%E5%B0%86%E5%A4%9A%E6%A8%A1%E6%80%81%E6%95%B0%E6%8D%AE%E8%9E%8D%E5%85%A5%E5%88%B0BERT%E6%9E%B6%E6%9E%84%E4%B8%AD-%E5%A4%9A%E6%A8%A1%E6%80%81BERT%E7%9A%84%E4%B8%A4%E7%B1%BB%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1\">如何将多模态数据融入到BERT架构中-多模态BERT的两类预训练任务</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%A4%9A%E6%A8%A1%E6%80%81/%E5%B1%82%E6%AC%A1%E5%88%86%E7%B1%BB%E4%BD%93%E7%B3%BB%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7-%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97\">层次分类体系的必要性-多模态讲解系列(1)</a></li>\n<li><a href=\"%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%A4%9A%E6%A8%A1%E6%80%81/%E6%96%87%E6%9C%AC%E5%92%8C%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%E6%A8%A1%E5%9D%97%E8%AF%A6%E8%A7%A3-%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%B2%E8%A7%A3%E7%B3%BB%E5%88%97\">文本和图像特征表示模块详解-多模态讲解系列(2)</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%A4%9A%E6%A8%A1%E6%80%81/%E5%A4%9A%E6%A8%A1%E6%80%81%E4%B8%AD%E5%90%84%E7%A7%8DFusion%E6%96%B9%E5%BC%8F%E6%B1%87%E6%80%BB\">多模态中各种Fusion方式汇总</a></li>\n</ol>\n<h3 id=\"句向量-sentence-embedding\">句向量-sentence embedding<a title=\"#句向量-sentence-embedding\" href=\"#句向量-sentence-embedding\"></a></h3>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%8F%A5%E5%90%91%E9%87%8F/%E5%8F%A5%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0\">句向量模型综述</a></li>\n</ol>\n<h3 id=\"文本相似度\">文本相似度<a title=\"#文本相似度\" href=\"#文本相似度\"></a></h3>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E5%92%8C%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/%E4%BA%94%E5%8D%83%E5%AD%97%E5%85%A8%E9%9D%A2%E6%A2%B3%E7%90%86%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%92%8C%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B\">五千字全面梳理文本相似度/文本匹配模型</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E5%92%8C%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/ESIM\">如何又好又快的做文本匹配-ESIM模型</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E5%92%8C%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/%E9%98%BF%E9%87%8CRE2-%E5%B0%86%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%92%8C%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88\">阿里RE2-将残差连接和文本匹配模型融合</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E5%92%8C%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/%E8%81%8A%E4%B8%80%E4%B8%8B%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C%E5%92%8CDSSM%E7%9A%84%E6%B7%B7%E6%B7%86%E7%82%B9%E4%BB%A5%E5%8F%8A%E5%90%91%E9%87%8F%E5%8F%AC%E5%9B%9E%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%86%E8%8A%82\">聊一下孪生网络和DSSM的混淆点以及向量召回的一个细节</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E5%92%8C%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/DSSM%E8%AE%BA%E6%96%87-%E5%85%AC%E5%8F%B8%E5%AE%9E%E6%88%98%E6%96%87%E7%AB%A0\">DSSM论文-公司实战文章</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E5%92%8C%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/bert%E7%99%BD%E5%8C%96%E7%AE%80%E5%8D%95%E7%9A%84%E6%A2%B3%E7%90%86\">bert白化简单的梳理:公式推导+PCA&amp;SVD+代码解读</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E5%92%8C%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/SIMCSE%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90\">SIMCSE论文解析</a></li>\n</ol>\n<h3 id=\"关键词提取\">关键词提取<a title=\"#关键词提取\" href=\"#关键词提取\"></a></h3>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/%E5%9F%BA%E4%BA%8E%E8%AF%8D%E5%85%B8%E7%9A%84%E6%AD%A3%E5%90%91%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E5%92%8C%E9%80%86%E5%90%91%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D\">基于词典的正向/逆向最大匹配</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/%E5%AE%9E%E4%BD%93%E5%BA%93%E6%9E%84%E5%BB%BA%EF%BC%9A%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%A6%BB%E7%BA%BF%E6%96%B0%E8%AF%8D%E5%AE%9E%E4%BD%93%E6%8C%96%E6%8E%98\">实体库构建：大规模离线新词实体挖掘</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0\">聊一聊NLPer如何做关键词抽取</a></li>\n</ol>\n<h3 id=\"命名体识别\">命名体识别<a title=\"#命名体识别\" href=\"#命名体识别\"></a></h3>\n<ol>\n<li>\n<p><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%91%BD%E5%90%8D%E4%BD%93%E8%AF%86%E5%88%AB/%E5%91%BD%E5%90%8D%E4%BD%93%E8%AF%86%E5%88%AB%E8%B5%84%E6%BA%90%E6%A2%B3%E7%90%86%E4%BB%A3%E7%A0%81%2B%E5%8D%9A%E5%AE%A2%E8%AE%B2%E8%A7%A3\">命名体识别资源梳理(代码+博客讲解)</a></p>\n</li>\n<li>\n<p><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%91%BD%E5%90%8D%E4%BD%93%E8%AF%86%E5%88%AB/HMM_CRF\">HMM/CRF 详细解读</a></p>\n</li>\n<li>\n<p><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%91%BD%E5%90%8D%E4%BD%93%E8%AF%86%E5%88%AB/%E5%B7%A5%E4%B8%9A%E7%BA%A7%E5%91%BD%E5%90%8D%E4%BD%93%E8%AF%86%E5%88%AB%E7%9A%84%E5%81%9A%E6%B3%95\">工业级命名体识别的做法</a></p>\n</li>\n<li>\n<p><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%91%BD%E5%90%8D%E4%BD%93%E8%AF%86%E5%88%AB/%E8%AF%8D%E5%85%B8%E5%8C%B9%E9%85%8D%2B%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B-%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E4%B8%A4%E5%A4%A7%E6%B3%95%E5%AE%9D\">词典匹配+模型预测-实体识别两大法宝</a></p>\n</li>\n<li>\n<p><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%91%BD%E5%90%8D%E4%BD%93%E8%AF%86%E5%88%AB/autoner\">autoner+fuzzy-CRF-使用领域词典做命名体识别</a></p>\n</li>\n<li>\n<p><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%91%BD%E5%90%8D%E4%BD%93%E8%AF%86%E5%88%AB/FLAT-Transformer\">FLAT-Transformer-词典+Transformer融合词汇信息</a>–公众号</p>\n</li>\n<li>\n<p><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%91%BD%E5%90%8D%E4%BD%93%E8%AF%86%E5%88%AB/TNER-%E5%A4%8D%E6%97%A6%E4%B8%BA%E4%BB%80%E4%B9%88TRM%E5%9C%A8NER%E4%B8%8A%E6%95%88%E6%9E%9C%E5%B7%AE\">TENER-复旦为什么TRM在NER上效果差</a></p>\n</li>\n</ol>\n<h3 id=\"文本分类\">文本分类<a title=\"#文本分类\" href=\"#文本分类\"></a></h3>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/CNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E8%A7%A3%E8%AF%BB\">TextCNN论文详细解读</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/%E5%8F%AA%E4%BD%BF%E7%94%A8%E6%A0%87%E7%AD%BE%E5%90%8D%E7%A7%B0%E5%B0%B1%E5%8F%AF%E4%BB%A5%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB\">只使用标签名称就可以文本分类 </a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%85%A5%E9%97%A8%E6%80%9D%E6%83%B3%E4%B9%8B%E4%BC%AA%E6%A0%87%E7%AD%BE\">半监督入门思想之伪标签</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/ACL2020-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E8%B4%9F%E7%9B%91%E7%9D%A3%E6%96%B9%E5%BC%8F%E5%A2%9E%E5%8A%A0CLS%E8%A1%A8%E8%BE%BE%E5%B7%AE%E5%BC%82%E6%80%A7\">ACL2020-多任务负监督方式增加CLS表达差异性</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/%E5%9C%A8%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8A%E5%BE%AE%E8%B0%83Bert\">Bert在文本分类任务上微调</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/UDA\">UDA-Unsupervised Data Augmentation for Consistency Training-半监督集大成</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/LCM-%E7%BC%93%E8%A7%A3%E6%A0%87%E7%AD%BE%E4%B8%8D%E7%8B%AC%E7%AB%8B%E4%BB%A5%E5%8F%8A%E6%A0%87%E6%B3%A8%E9%94%99%E8%AF%AF%E7%9A%84%E9%97%AE%E9%A2%98\">LCM-缓解标签不独立以及标注错误的问题</a></li>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/%E5%85%B3%E9%94%AE%E8%AF%8D%E4%BF%A1%E6%81%AF%E5%A6%82%E4%BD%95%E8%9E%8D%E5%85%A5%E5%88%B0%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E4%B8%AD\">关键词信息如何融入到文本分类任务中</a></li>\n</ol>\n<h3 id=\"对比学习\">对比学习<a title=\"#对比学习\" href=\"#对比学习\"></a></h3>\n<ol>\n<li><a href=\"./links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/Moco1%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90\">Moco论文解析</a></li>\n</ol>\n","link":"links/NLP_ability","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/","toc":[{"id":"背景介绍","title":"背景介绍","index":"1","children":[{"id":"深度学习自然语言处理","title":"深度学习自然语言处理","index":"1.1","children":[{"id":"transformer","title":"Transformer","index":"1.1.1"},{"id":"bert-基本知识","title":"Bert-基本知识","index":"1.1.2"},{"id":"bert-知识蒸馏","title":"Bert-知识蒸馏","index":"1.1.3"},{"id":"词向量-word-embedding","title":"词向量-word embedding","index":"1.1.4"},{"id":"多模态","title":"多模态","index":"1.1.5"},{"id":"句向量-sentence-embedding","title":"句向量-sentence embedding","index":"1.1.6"},{"id":"文本相似度","title":"文本相似度","index":"1.1.7"},{"id":"关键词提取","title":"关键词提取","index":"1.1.8"},{"id":"命名体识别","title":"命名体识别","index":"1.1.9"},{"id":"文本分类","title":"文本分类","index":"1.1.10"},{"id":"对比学习","title":"对比学习","index":"1.1.11"}]}]}],"reward":true}