{"title":"","date":"2025-12-02T22:37:32.897Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:37:32.897Z","content":"<p>pytorch处理文本数据代码版本1-处理文本相似度数据</p>\n<p>下面的代码，相比于版本2的代码，并没有使用gensim，而且处理的时候针对的是每一个样本，也就是每一行，也就是<br>\nsentence1和sentence2并没有拆开来处理。</p>\n<p>整体代码是我自己完全整理出来的，比较整齐</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">@author: DASOU</span></span><br><span class=\"line\"><span class=\"string\">@time: 20200726</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle <span class=\"keyword\">as</span> pkl</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 读取原始数据，生成对应的word2index</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_word_voc</span>(<span class=\"params\">config_base</span>):</span><br><span class=\"line\">    train_path=config_base.train_path</span><br><span class=\"line\">    file=<span class=\"built_in\">open</span>(train_path,<span class=\"string\">&#x27;r&#x27;</span>)</span><br><span class=\"line\">    lines=file.readlines()</span><br><span class=\"line\">    min_freq,max_size,UNK,PAD=config_base.min_freq,config_base.max_size,config_base.UNK,config_base.PAD</span><br><span class=\"line\">    vocab_dic=&#123;&#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            line=line.strip().split(<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">except</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;The data formate is not correct,please correct it as example data&#x27;</span>)</span><br><span class=\"line\">            exit()</span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(line)==<span class=\"number\">3</span>:</span><br><span class=\"line\">                sen=line[<span class=\"number\">0</span>]+line[<span class=\"number\">1</span>]</span><br><span class=\"line\">                tokenizer = <span class=\"keyword\">lambda</span> x: [y <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> x]</span><br><span class=\"line\">                <span class=\"keyword\">for</span> word <span class=\"keyword\">in</span> tokenizer(sen):</span><br><span class=\"line\">                    vocab_dic[word] = vocab_dic.get(word, <span class=\"number\">0</span>) + <span class=\"number\">1</span> <span class=\"comment\">## 为了计算出每个单词的词频，为之后过滤低频词汇做准备</span></span><br><span class=\"line\">        <span class=\"keyword\">except</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;The data formate is not correct,please correct it as example data&#x27;</span>)</span><br><span class=\"line\">            exit()</span><br><span class=\"line\">    file.close()</span><br><span class=\"line\">    vocab_list = <span class=\"built_in\">sorted</span>([_ <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> vocab_dic.items() <span class=\"keyword\">if</span> _[<span class=\"number\">1</span>] &gt;= min_freq], key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)[:max_size]<span class=\"comment\">## 是为了计算每个单词的词频</span></span><br><span class=\"line\">    vocab_dic = &#123;word_count[<span class=\"number\">0</span>]: idx <span class=\"keyword\">for</span> idx, word_count <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(vocab_list)&#125;<span class=\"comment\">## 过滤掉低频词汇之后我们按照顺序来word-index的映射</span></span><br><span class=\"line\">    vocab_dic.update(&#123;UNK: <span class=\"built_in\">len</span>(vocab_dic), PAD: <span class=\"built_in\">len</span>(vocab_dic) + <span class=\"number\">1</span>&#125;) <span class=\"comment\">## 补充unkonw和pad字符对应的数字</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> vocab_dic</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">load_data</span>(<span class=\"params\">cate,vocab_dic,config_base</span>):</span><br><span class=\"line\">    <span class=\"keyword\">if</span> cate==<span class=\"string\">&#x27;train&#x27;</span>:</span><br><span class=\"line\">        data_path=config_base.train_path</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> cate==<span class=\"string\">&#x27;dev&#x27;</span>:</span><br><span class=\"line\">        data_path = config_base.dev_path</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        data_path = config_base.test_path</span><br><span class=\"line\">    file=<span class=\"built_in\">open</span>(data_path,<span class=\"string\">&#x27;r&#x27;</span>)</span><br><span class=\"line\">    contents=[]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> file.readlines():</span><br><span class=\"line\">        words_line1=[]</span><br><span class=\"line\">        words_line2=[]</span><br><span class=\"line\">        line=line.strip().split(<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">        sen1,sen2,label=line[<span class=\"number\">0</span>],line[<span class=\"number\">1</span>],line[<span class=\"number\">2</span>]</span><br><span class=\"line\">        tokenizer = <span class=\"keyword\">lambda</span> x: [y <span class=\"keyword\">for</span> y <span class=\"keyword\">in</span> x]</span><br><span class=\"line\">        token_sen1=tokenizer(sen1)</span><br><span class=\"line\">        token_sen2 = tokenizer(sen2)</span><br><span class=\"line\">        sen1_len = <span class=\"built_in\">len</span>(token_sen1)</span><br><span class=\"line\">        sen2_len = <span class=\"built_in\">len</span>(token_sen2)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> config_base.pad_size:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(token_sen1) &lt; config_base.pad_size:</span><br><span class=\"line\">                token_sen1.extend([config_base.PAD] * (config_base.pad_size - <span class=\"built_in\">len</span>(token_sen1)))</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                token_sen1 = token_sen1[:config_base.pad_size]</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(token_sen2) &lt; config_base.pad_size:</span><br><span class=\"line\">                token_sen2.extend([config_base.PAD] * (config_base.pad_size - <span class=\"built_in\">len</span>(token_sen2)))</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                token_sen2 = token_sen2[:config_base.pad_size]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> word1 <span class=\"keyword\">in</span> token_sen1:</span><br><span class=\"line\">            words_line1.append(vocab_dic.get(word1, vocab_dic.get(config_base.UNK)))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> word2 <span class=\"keyword\">in</span> token_sen2:</span><br><span class=\"line\">            words_line2.append(vocab_dic.get(word2, vocab_dic.get(config_base.UNK)))</span><br><span class=\"line\">        contents.append((words_line1,words_line2,<span class=\"built_in\">int</span>(label)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> contents</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 导入/训练对应的word2index</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_w2i</span>(<span class=\"params\">config_base</span>):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.exists(config_base.w2i_path):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;There is not a pre word2index,now is to process data for geting word2index&#x27;</span>)</span><br><span class=\"line\">        vocab_dic = get_word_voc(config_base)</span><br><span class=\"line\">        pkl.dump(vocab_dic, <span class=\"built_in\">open</span>(config_base.w2i_path, <span class=\"string\">&#x27;wb&#x27;</span>))</span><br><span class=\"line\">        vord_size = <span class=\"built_in\">len</span>(vocab_dic)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;There is pre word2index, now is to load the pre infomation&#x27;</span>)</span><br><span class=\"line\">        vocab_dic = pkl.load(<span class=\"built_in\">open</span>(config_base.w2i_path, <span class=\"string\">&#x27;rb&#x27;</span>), encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>)</span><br><span class=\"line\">        vord_size = <span class=\"built_in\">len</span>(vocab_dic)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> vocab_dic,vord_size</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">DatasetIterater</span>():</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, batches, config_base</span>):</span><br><span class=\"line\">        self.batch_size = config_base.batch_size</span><br><span class=\"line\">        self.batches = batches</span><br><span class=\"line\">        self.n_batches = <span class=\"built_in\">len</span>(batches) // config_base.batch_size</span><br><span class=\"line\">        self.residue = <span class=\"literal\">False</span>  <span class=\"comment\"># 记录batch数量是否为整数</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(batches) % self.n_batches != <span class=\"number\">0</span>:</span><br><span class=\"line\">            self.residue = <span class=\"literal\">True</span></span><br><span class=\"line\">        self.index = <span class=\"number\">0</span></span><br><span class=\"line\">        self.device = config_base.device</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_to_tensor</span>(<span class=\"params\">self, datas</span>):</span><br><span class=\"line\">        x1 = torch.LongTensor([_[<span class=\"number\">0</span>] <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> datas]).to(self.device)</span><br><span class=\"line\">        x2 = torch.LongTensor([_[<span class=\"number\">1</span>] <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> datas]).to(self.device)</span><br><span class=\"line\">        y = torch.LongTensor([_[<span class=\"number\">2</span>] <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> datas]).to(self.device)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (x1, x2), y</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__next__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.residue <span class=\"keyword\">and</span> self.index == self.n_batches:</span><br><span class=\"line\">            batches = self.batches[self.index * self.batch_size: <span class=\"built_in\">len</span>(self.batches)]</span><br><span class=\"line\">            self.index += <span class=\"number\">1</span></span><br><span class=\"line\">            batches = self._to_tensor(batches)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> batches</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">elif</span> self.index &gt;= self.n_batches:</span><br><span class=\"line\">            self.index = <span class=\"number\">0</span></span><br><span class=\"line\">            <span class=\"keyword\">raise</span> StopIteration</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            batches = self.batches[self.index * self.batch_size: (self.index + <span class=\"number\">1</span>) * self.batch_size]</span><br><span class=\"line\">            self.index += <span class=\"number\">1</span></span><br><span class=\"line\">            batches = self._to_tensor(batches)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> batches</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__iter__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__len__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> self.residue:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.n_batches + <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.n_batches</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">build_iterator</span>(<span class=\"params\">dataset,config_base</span>):</span><br><span class=\"line\">    <span class=\"built_in\">iter</span> = DatasetIterater(dataset,config_base)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">iter</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>","link":"links/NLP_ability/Pytorch/Pytorch/pytorch处理文本数据代码版本1-处理文本相似度数据","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/Pytorch/pytorch处理文本数据代码版本1-处理文本相似度数据/","reward":true}