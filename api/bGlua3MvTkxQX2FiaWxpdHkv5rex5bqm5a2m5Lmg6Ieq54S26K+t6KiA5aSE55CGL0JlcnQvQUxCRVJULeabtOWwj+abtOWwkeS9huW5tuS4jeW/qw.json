{"title":"","date":"2025-12-02T22:25:48.577Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:25:48.577Z","content":"<p>BERT模型的压缩大致可以分为：1. 参数剪枝；2. 知识蒸馏；3. 参数共享；4. 低秩分解。</p>\n<p>其中，对于剪枝，比较简单，但是容易误操作降低精读；</p>\n<p>对于知识蒸馏，之前我写个一系列的文章，重点可以看一下这里：</p>\n<p>对于参数共享和低秩分解，就和今天分享的<a href=\"https://arxiv.org/pdf/1909.11942.pdf,\" title=\"ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS\" target=\"_blank\">ALBERT</a>息息相关；</p>\n<p>它减少了BERT的参数，但是需要注意的一个细节点是，同等规格下，ALBERT速度确实变快，但是并不明显（和大量自媒体文章解读给大家的印象差距很大）；</p>\n<p>举个形象的例子就是，（这个例子并不严谨，只是帮助理解）参数共享让它训练的时候把多层压缩为一层去训练，但是在预测的时候，我们需要再展开多层去进行预测。</p>\n<p>主要掌握以下的几个知识点：</p>\n<ol>\n<li>词向量嵌入参数分解</li>\n<li>跨层参数分享</li>\n<li>取消NSP，使用SOP</li>\n<li>预训练的时候采用更满的数据/n-gram mask方式</li>\n</ol>\n<h1 id=\"1.词向量嵌入分解\">1.词向量嵌入分解<a title=\"#1.词向量嵌入分解\" href=\"#1.词向量嵌入分解\"></a></h1>\n<p>词向量嵌入参数分解，简单说就是将词向量矩阵分解为了两个小矩阵，将隐藏层的大小和词汇矩阵的大小分离开。</p>\n<p>在Bert中，词汇表embedding大小是$V*H$；</p>\n<p>Albert 的参数分解是这样的，将这个矩阵分解为两个小矩阵：$V<em>E$和$E</em>H$</p>\n<p>这样做有什么好处呢？</p>\n<p>如果说，我觉得我的模型表达能力不够，我想要通过增大隐层H的大小来提升我们模型能力的表达能力，那么在提升H的时候，不仅仅隐层参数增多，词汇表的embedding矩阵维度也在增多，参数量也在增大。</p>\n<p>矩阵分解之后，我们可以只是做到提升隐层大小，而不去改变表词汇表的大小。</p>\n<h1 id=\"2.跨层参数分享\">2.跨层参数分享<a title=\"#2.跨层参数分享\" href=\"#2.跨层参数分享\"></a></h1>\n<p>跨层参数分享，这个操作可以防止参数随着网络层数的增大而增加。</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-04-063530.jpg\" alt=\"跨层参数共享\" loading=\"lazy\" class=\"φbp\"></p>\n<p>分为三种形式，只是共享attentions，只是共享FFN，全部共享。</p>\n<p>共享的意思就是我这部分结构只使用同样的参数，在训练的时候只需要训练这一部分的参数就可以了。</p>\n<p>看表格我们可以发现一个细节，就是只是共享FFN比只是共享attention的参数，模型效果要降低的多。</p>\n<p>小声嘀咕一下，这是不是说明FFN比attention在信息表达上要重要啊。或者说attention在学习信息表达的时候。attention层学习共性比较多。FFN学习到的差异性比较多。这只是我自己的猜测哈。</p>\n<h1 id=\"3.-sop\">3. SOP<a title=\"#3.-sop\" href=\"#3.-sop\"></a></h1>\n<p>作者认为，NSP不必要。与MLM相比，NSP失效的主要原因是其缺乏任务难度。</p>\n<p>NSP样本如下:</p>\n<ul>\n<li>从训练语料库中取出两个连续的段落作为正样本</li>\n<li>从不同的文档中随机创建一对段落作为负样本</li>\n</ul>\n<p>NSP将主题预测和连贯性预测合并为一个单项任务；</p>\n<p>但是，与连贯性预测相比，主题预测更容易学习，并且与使用MLM损失学习的内容相比，重叠性更大。</p>\n<p>对于ALBERT，作者使用了句子顺序预测（SOP）损失，它避免了主题预测，而是着重于句间建模。</p>\n<p>其实就是预测句子顺序，正样本是顺着，负样本是颠倒过来。都是来自同一个文档。</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-04-063528.jpg\" alt=\"SOP\" loading=\"lazy\" class=\"φbp\"></p>\n<h1 id=\"其他细节\">其他细节<a title=\"#其他细节\" href=\"#其他细节\"></a></h1>\n<ol>\n<li>数据格式：<strong>Segments-Pair</strong></li>\n</ol>\n<p>这个在RoBERTa中也有谈到，更长的序列长度可以提升性能。</p>\n<ol start=\"2\">\n<li>Masked-ngram-LM</li>\n</ol>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-04-063529.jpg\" alt=\"Masked-ngram-LM\" loading=\"lazy\" class=\"φbp\"></p>\n<p>这就有点类似百度的ERINE和SpanBERT了</p>\n<ol start=\"3\">\n<li>推测速度</li>\n</ol>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-04-064022.png\" alt=\"ALBERT推理速度\" loading=\"lazy\" class=\"φbp\"></p>\n<p>从图中知道，同一规模ALBERT和BERT，比如同为Base：</p>\n<p>BERT base: 4.7x；ALBERT base：5.6x；<strong>速度确实变快，但是确实加速并不明显</strong>；</p>\n<p>同等效果的情况下，比如BERT base（Avg=82.3）和ALBERT large（Avg=82.4）：</p>\n<p>BERT base：4.7x；ALBERT large：1.7x；速度变慢了</p>\n<h1 id=\"总结\">总结<a title=\"#总结\" href=\"#总结\"></a></h1>\n<p>总结一下可以学习的思路：</p>\n<ol>\n<li>预训练的时候，数据填充的更满，到512这种，有利于提升模型效果，这点在RoBERTa有谈到</li>\n<li>mask n-gram有利于提升效果，这点类似百度的ERINE和SpanBERT了</li>\n<li>词向量矩阵分解能减少参数，但是也会降低性能</li>\n<li>跨层参数分享可以降低参数，也会降低性能，通过实验图知道，attention共享效果还好，FFN共享效果降低有点多</li>\n<li>取消NSP，使用SOP，正负样本来自同一个文档，但是顺序不同。</li>\n<li><strong>推理速度来看，同等规格，ALBERT速度确实变快，但是并不明显，同等效果，速度变慢</strong>；<a href=\"https://kexue.fm/archives/7846\" target=\"_blank\">https://kexue.fm/archives/7846</a>)</li>\n</ol>\n","link":"links/NLP_ability/深度学习自然语言处理/Bert/ALBERT-更小更少但并不快","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/ALBERT-更小更少但并不快/","toc":[{"id":"1.词向量嵌入分解","title":"1.词向量嵌入分解","index":"1"},{"id":"2.跨层参数分享","title":"2.跨层参数分享","index":"2"},{"id":"3.-sop","title":"3. SOP","index":"3"},{"id":"其他细节","title":"其他细节","index":"4"},{"id":"总结","title":"总结","index":"5"}],"reward":true}