{"title":"","date":"2025-12-02T22:25:48.728Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:25:48.728Z","content":"<p>假如手上有一个文本分类任务，我们在提升模型效果的时候一般有以下几个思路：</p>\n<ol>\n<li>\n<p>增大数据集，同时提升标注质量</p>\n</li>\n<li>\n<p>寻找更多有效的文本特征，比如词性特征，词边界特征等等</p>\n</li>\n<li>\n<p>更换模型，使用更加适合当前任务或者说更加复杂的模型，比如FastText–&gt;TextCNN–Bert</p>\n</li>\n</ol>\n<p>…</p>\n<p>之后接触到了知识蒸馏，学习到了简单的神经网络可以从复杂的网路中学习知识，进而提升模型效果。</p>\n<p>之前写个一个文章是TextCNN如何逼近Bert，当时写得比较粗糙，但是比较核心的点已经写出来。</p>\n<p>这个文章脱胎于这个论文：Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</p>\n<p>整个训练过程是这样的：</p>\n<ol>\n<li>在标签数据上微调Bert模型</li>\n<li>使用三种方式对无标签数据进行数据增强</li>\n<li>Bert模型在无标签数据上进行推理，Lstm模型学习Bert模型的推理结果，使用MSE作为损失函数。</li>\n</ol>\n<h4 id=\"目标函数\">目标函数<a title=\"#目标函数\" href=\"#目标函数\"></a></h4>\n<p>知识蒸馏的目标函数：</p>\n<p><img src=\"../images/bilstm%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png\" alt=\"bilstm损失函数\" loading=\"lazy\" class=\"φbp\"></p>\n<p>一般来说，我们会使用两个部分，一个是硬目标损失函数，一个是软目标损失函数，两者都可以使用交叉熵进行度量。</p>\n<p>在原论文中，作者在计算损失函数的时候只是使用到了软目标，同时这个软目标并不是使用softmax之前的logits进行MSE度量损失，也就是并没有使用带有温度参数T的sotmax进行归一化。</p>\n<h4 id=\"数据增强\">数据增强<a title=\"#数据增强\" href=\"#数据增强\"></a></h4>\n<p>为了促进有效的知识转移，我们经常需要一个庞大的，未标记的数据集。</p>\n<p>三种数据增强的方式：</p>\n<ol>\n<li>\n<p>Masking：使用概率$P_{mask}$随机的替换一个单词为[MASK].</p>\n<p>需要注意的是这里替换之后，Bert模型也会输入这个数据的。从直觉上来讲，这个规则可以阐明每个单词对标签的影响。</p>\n</li>\n<li>\n<p>POS-guided word replacement.使用概率$P_{pos}$随机替换一个单词为另一个相同POS的单词。这个规则有可能会改变句子的语义信息。</p>\n</li>\n<li>\n<p>n-gram sampling</p>\n</li>\n</ol>\n<p>整个流程是这样的：对于每个单词，如果概率p&lt;$p_{mask}$，我们使用第一条规则，如果p&lt;$p_{mask}+p_{pos}$，我们使用第二条规则，两条规则互斥，也就是同一个单词只使用两者之间的一个。当对句子中的每个单词都过了一遍之后，我进行第三条规则，之后把整条句子补充道无标签数据集中。</p>\n<h4 id=\"知识蒸馏结果图\">知识蒸馏结果图<a title=\"#知识蒸馏结果图\" href=\"#知识蒸馏结果图\"></a></h4>\n<p>效果图：</p>\n<p><img src=\"../images/lstm%E8%92%B8%E9%A6%8F%E6%95%88%E6%9E%9C%E5%9B%BE.png\" alt=\"lstm蒸馏效果图\" loading=\"lazy\" class=\"φbp\"></p>\n","link":"links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/模型蒸馏/Bert蒸馏到简单网络lstm","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/模型蒸馏/Bert蒸馏到简单网络lstm/","reward":true}