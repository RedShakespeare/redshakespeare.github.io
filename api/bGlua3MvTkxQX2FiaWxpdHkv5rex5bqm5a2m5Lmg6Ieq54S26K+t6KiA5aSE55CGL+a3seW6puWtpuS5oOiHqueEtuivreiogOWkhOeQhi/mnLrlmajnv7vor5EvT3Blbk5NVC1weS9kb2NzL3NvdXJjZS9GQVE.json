{"title":"","date":"2025-12-02T22:37:33.054Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:37:33.054Z","content":"<h1 id=\"faq\">FAQ<a title=\"#faq\" href=\"#faq\"></a></h1>\n<h2 id=\"how-do-i-use-pretrained-embeddings-(e.g.-glove)?\">How do I use Pretrained embeddings (e.g. GloVe)?<a title=\"#how-do-i-use-pretrained-embeddings-(e.g.-glove)?\" href=\"#how-do-i-use-pretrained-embeddings-(e.g.-glove)?\"></a></h2>\n<p>Using vocabularies from OpenNMT-py preprocessing outputs, <code>embeddings_to_torch.py</code> to generate encoder and decoder embeddings initialized with GloVe’s values.</p>\n<p>the script is a slightly modified version of ylhsieh’s one2.</p>\n<p>Usage:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">embeddings_to_torch.py [-h] [-emb_file_both EMB_FILE_BOTH]</span><br><span class=\"line\">                       [-emb_file_enc EMB_FILE_ENC]</span><br><span class=\"line\">                       [-emb_file_dec EMB_FILE_DEC] -output_file</span><br><span class=\"line\">                       OUTPUT_FILE -dict_file DICT_FILE [-verbose]</span><br><span class=\"line\">                       [-skip_lines SKIP_LINES]</span><br><span class=\"line\">                       [-type &#123;GloVe,word2vec&#125;]</span><br></pre></td></tr></table></figure>\n<p>Run embeddings_to_torch.py -h for more usagecomplete info.</p>\n<p>Example</p>\n<ol>\n<li>get GloVe files:</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir &quot;glove_dir&quot;</span><br><span class=\"line\">wget http://nlp.stanford.edu/data/glove.6B.zip</span><br><span class=\"line\">unzip glove.6B.zip -d &quot;glove_dir&quot;</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>prepare data:</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_preprocess \\</span><br><span class=\"line\">-train_src data/train.src.txt \\</span><br><span class=\"line\">-train_tgt data/train.tgt.txt \\</span><br><span class=\"line\">-valid_src data/valid.src.txt \\</span><br><span class=\"line\">-valid_tgt data/valid.tgt.txt \\</span><br><span class=\"line\">-save_data data/data</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>prepare embeddings:</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./tools/embeddings_to_torch.py -emb_file_both &quot;glove_dir/glove.6B.100d.txt&quot; \\</span><br><span class=\"line\">-dict_file &quot;data/data.vocab.pt&quot; \\</span><br><span class=\"line\">-output_file &quot;data/embeddings&quot;</span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>train using pre-trained embeddings:</li>\n</ol>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_train -save_model data/model \\</span><br><span class=\"line\">           -batch_size 64 \\</span><br><span class=\"line\">           -layers 2 \\</span><br><span class=\"line\">           -rnn_size 200 \\</span><br><span class=\"line\">           -word_vec_size 100 \\</span><br><span class=\"line\">           -pre_word_vecs_enc &quot;data/embeddings.enc.pt&quot; \\</span><br><span class=\"line\">           -pre_word_vecs_dec &quot;data/embeddings.dec.pt&quot; \\</span><br><span class=\"line\">           -data data/data</span><br></pre></td></tr></table></figure>\n<h2 id=\"how-do-i-use-the-transformer-model?\">How do I use the Transformer model?<a title=\"#how-do-i-use-the-transformer-model?\" href=\"#how-do-i-use-the-transformer-model?\"></a></h2>\n<p>The transformer model is very sensitive to hyperparameters. To run it<br>\neffectively you need to set a bunch of different options that mimic the Google<br>\nsetup. We have confirmed the following command can replicate their WMT results.</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python  train.py -data /tmp/de2/data -save_model /tmp/extra \\</span><br><span class=\"line\">        -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  \\</span><br><span class=\"line\">        -encoder_type transformer -decoder_type transformer -position_encoding \\</span><br><span class=\"line\">        -train_steps 200000  -max_generator_batches 2 -dropout 0.1 \\</span><br><span class=\"line\">        -batch_size 4096 -batch_type tokens -normalization tokens  -accum_count 2 \\</span><br><span class=\"line\">        -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 \\</span><br><span class=\"line\">        -max_grad_norm 0 -param_init 0  -param_init_glorot \\</span><br><span class=\"line\">        -label_smoothing 0.1 -valid_steps 10000 -save_checkpoint_steps 10000 \\</span><br><span class=\"line\">        -world_size 4 -gpu_ranks 0 1 2 3</span><br></pre></td></tr></table></figure>\n<p>Here are what each of the parameters mean:</p>\n<ul>\n<li><code>param_init_glorot</code> <code>-param_init 0</code>: correct initialization of parameters</li>\n<li><code>position_encoding</code>: add sinusoidal position encoding to each embedding</li>\n<li><code>optim adam</code>, <code>decay_method noam</code>, <code>warmup_steps 8000</code>: use special learning rate.</li>\n<li><code>batch_type tokens</code>, <code>normalization tokens</code>, <code>accum_count 4</code>: batch and normalize based on number of tokens and not sentences. Compute gradients based on four batches.</li>\n<li><code>label_smoothing 0.1</code>: use label smoothing loss.</li>\n</ul>\n<h2 id=\"do-you-support-multi-gpu?\">Do you support multi-gpu?<a title=\"#do-you-support-multi-gpu?\" href=\"#do-you-support-multi-gpu?\"></a></h2>\n<p>First you need to make sure you <code>export CUDA_VISIBLE_DEVICES=0,1,2,3</code>.</p>\n<p>If you want to use GPU id 1 and 3 of your OS, you will need to <code>export CUDA_VISIBLE_DEVICES=1,3</code></p>\n<p>Both <code>-world_size</code> and <code>-gpu_ranks</code> need to be set. E.g. <code>-world_size 4 -gpu_ranks 0 1 2 3</code> will use 4 GPU on this node only.</p>\n<p>If you want to use 2 nodes with 2 GPU each, you need to set <code>-master_ip</code> and <code>-master_port</code>, and</p>\n<ul>\n<li><code>-world_size 4 -gpu_ranks 0 1</code>: on the first node</li>\n<li><code>-world_size 4 -gpu_ranks 2 3</code>: on the second node</li>\n<li><code>-accum_count 2</code>: This will accumulate over 2 batches before updating parameters.</li>\n</ul>\n<p>if you use a regular network card (1 Gbps) then we suggest to use a higher <code>-accum_count</code> to minimize the inter-node communication.</p>\n<p><strong>Note:</strong></p>\n<p>When training on several GPUs, you can’t have them in ‘Exclusive’ compute mode (<code>nvidia-smi -c 3</code>).</p>\n<p>The multi-gpu setup relies on a Producer/Consumer setup. This setup means there will be <code>2&lt;n_gpu&gt; + 1</code> processes spawned, with 2 processes per GPU, one for model training and one (Consumer) that hosts a <code>Queue</code> of batches that will be processed next. The additional process is the Producer, creating batches and sending them to the Consumers. This setup is beneficial for both wall time and memory, since it loads data shards ‘in advance’, and does not require to load it for each GPU process.</p>\n<h2 id=\"how-can-i-ensemble-models-at-inference?\">How can I ensemble Models at inference?<a title=\"#how-can-i-ensemble-models-at-inference?\" href=\"#how-can-i-ensemble-models-at-inference?\"></a></h2>\n<p>You can specify several models in the <a href=\"http://translate.py\">translate.py</a> command line: -model model1_seed1 model2_seed2<br>\nBear in mind that your models must share the same target vocabulary.</p>\n<h2 id=\"how-can-i-weight-different-corpora-at-training?\">How can I weight different corpora at training?<a title=\"#how-can-i-weight-different-corpora-at-training?\" href=\"#how-can-i-weight-different-corpora-at-training?\"></a></h2>\n<h3 id=\"preprocessing\">Preprocessing<a title=\"#preprocessing\" href=\"#preprocessing\"></a></h3>\n<p>We introduced <code>-train_ids</code> which is a list of IDs that will be given to the preprocessed shards.</p>\n<p>E.g. we have two corpora : <code>parallel.en</code> and  <code>parallel.de</code> + <code>from_backtranslation.en</code> <code>from_backtranslation.de</code>, we can pass the following in the <code>preprocess.py</code> command:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">-train_src parallel.en from_backtranslation.en \\</span><br><span class=\"line\">-train_tgt parallel.de from_backtranslation.de \\</span><br><span class=\"line\">-train_ids A B \\</span><br><span class=\"line\">-save_data my_data \\</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>and it will dump <code>my_data.train_A.X.pt</code> based on <code>parallel.en</code>//<code>parallel.de</code> and <code>my_data.train_B.X.pt</code> based on <code>from_backtranslation.en</code>//<code>from_backtranslation.de</code>.</p>\n<h3 id=\"training\">Training<a title=\"#training\" href=\"#training\"></a></h3>\n<p>We introduced <code>-data_ids</code> based on the same principle as above, as well as <code>-data_weights</code>, which is the list of the weight each corpus should have.<br>\nE.g.</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">-data my_data \\</span><br><span class=\"line\">-data_ids A B \\</span><br><span class=\"line\">-data_weights 1 7 \\</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>will mean that we’ll look for <code>my_data.train_A.*.pt</code> and <code>my_data.train_B.*.pt</code>, and that when building batches, we’ll take 1 example from corpus A, then 7 examples from corpus B, and so on.</p>\n<p><strong>Warning</strong>: This means that we’ll load as many shards as we have <code>-data_ids</code>, in order to produce batches containing data from every corpus. It may be a good idea to reduce the <code>-shard_size</code> at preprocessing.</p>\n<h2 id=\"can-i-get-word-alignment-while-translating?\">Can I get word alignment while translating?<a title=\"#can-i-get-word-alignment-while-translating?\" href=\"#can-i-get-word-alignment-while-translating?\"></a></h2>\n<h3 id=\"raw-alignments-from-averaging-transformer-attention-heads\">Raw alignments from averaging Transformer attention heads<a title=\"#raw-alignments-from-averaging-transformer-attention-heads\" href=\"#raw-alignments-from-averaging-transformer-attention-heads\"></a></h3>\n<p>Currently, we support producing word alignment while translating for Transformer based models. Using <code>-report_align</code> when calling <code>translate.py</code> will output the inferred alignments in Pharaoh format. Those alignments are computed from an argmax on the average of the attention heads of the <em>second to last</em> decoder layer. The resulting alignment src-tgt (Pharaoh) will be pasted to the translation sentence, separated by <code>|||</code>.<br>\nNote: The <em>second to last</em> default behaviour was empirically determined. It is not the same as the paper (they take the <em>penultimate</em> layer), probably because of light differences in the architecture.</p>\n<ul>\n<li>alignments use the standard “Pharaoh format”, where a pair <code>i-j</code> indicates the i<sub>th</sub> word of source language is aligned to j<sub>th</sub> word of target language.</li>\n<li>Example: {‘src’: ‘das stimmt nicht !’; ‘output’: ‘that is not true ! ||| 0-0 0-1 1-2 2-3 1-4 1-5 3-6’}</li>\n<li>Using the<code>-tgt</code> option when calling <code>translate.py</code>, we output alignments between the source and the gold target rather than the inferred target, assuming we’re doing evaluation.</li>\n<li>To convert subword alignments to word alignments, or symetrize bidirectional alignments, please refer to the <a href=\"https://github.com/lilt/alignment-scripts\" target=\"_blank\">lilt scripts</a>.</li>\n</ul>\n<h3 id=\"supervised-learning-on-a-specific-head\">Supervised learning on a specific head<a title=\"#supervised-learning-on-a-specific-head\" href=\"#supervised-learning-on-a-specific-head\"></a></h3>\n<p>The quality of output alignments can be further improved by providing reference alignments while training. This will invoke multi-task learning on translation and alignment. This is an implementation based on the paper <a href=\"https://arxiv.org/abs/1909.02074\" target=\"_blank\">Jointly Learning to Align and Translate with Transformer Models</a>.</p>\n<p>The data need to be preprocessed with the reference alignments in order to learn the supervised task.</p>\n<p>When calling <code>preprocess.py</code>, add:</p>\n<ul>\n<li><code>--train_align &lt;path&gt;</code>: path(s) to the training alignments in Pharaoh format</li>\n<li><code>--valid_align &lt;path&gt;</code>: path to the validation set alignments in Pharaoh format (optional).<br>\nThe reference alignment file(s) could be generated by <a href=\"https://github.com/moses-smt/mgiza/\" target=\"_blank\">GIZA++</a> or <a href=\"https://github.com/clab/fast_align\" target=\"_blank\">fast_align</a>.</li>\n</ul>\n<p>Note: There should be no blank lines in the alignment files provided.</p>\n<p>Options to learn such alignments are:</p>\n<ul>\n<li><code>-lambda_align</code>: set the value &gt; 0.0 to enable joint align training, the paper suggests 0.05;</li>\n<li><code>-alignment_layer</code>: indicate the index of the decoder layer;</li>\n<li><code>-alignment_heads</code>:  number of alignment heads for the alignment task - should be set to 1 for the supervised task, and preferably kept to default (or same as <code>num_heads</code>) for the average task;</li>\n<li><code>-full_context_alignment</code>: do full context decoder pass (no future mask) when computing alignments. This will slow down the training (~12% in terms of tok/s) but will be beneficial to generate better alignment.</li>\n</ul>\n","link":"links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/FAQ","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/FAQ/","toc":[{"id":"faq","title":"FAQ","index":"1","children":[{"id":"how-do-i-use-pretrained-embeddings-(e.g.-glove)?","title":"How do I use Pretrained embeddings (e.g. GloVe)?","index":"1.1"},{"id":"how-do-i-use-the-transformer-model?","title":"How do I use the Transformer model?","index":"1.2"},{"id":"do-you-support-multi-gpu?","title":"Do you support multi-gpu?","index":"1.3"},{"id":"how-can-i-ensemble-models-at-inference?","title":"How can I ensemble Models at inference?","index":"1.4"},{"id":"how-can-i-weight-different-corpora-at-training?","title":"How can I weight different corpora at training?","index":"1.5","children":[{"id":"preprocessing","title":"Preprocessing","index":"1.5.1"},{"id":"training","title":"Training","index":"1.5.2"}]},{"id":"can-i-get-word-alignment-while-translating?","title":"Can I get word alignment while translating?","index":"1.6","children":[{"id":"raw-alignments-from-averaging-transformer-attention-heads","title":"Raw alignments from averaging Transformer attention heads","index":"1.6.1"},{"id":"supervised-learning-on-a-specific-head","title":"Supervised learning on a specific head","index":"1.6.2"}]}]}],"reward":true}