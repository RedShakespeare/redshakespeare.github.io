{"title":"","date":"2025-12-02T22:16:22.007Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:16:22.007Z","content":"<p><strong>如果面试官问【聊一下RNN中的梯度消失】</strong></p>\n<p>盲猜很多同学的回答可以简化成这样形式【由于网络太深，梯度反向传播会出现连乘效应，从而出现梯度消失】</p>\n<p>这样的回答，如果用在普通网络，类似MLP，是没有什么问题的，但是放在RNN中，是错误的。</p>\n<p><strong>RNN的梯度是一个和，是近距离梯度和远距离梯度的和；</strong></p>\n<p>RNN中的梯度消失的含义是远距离的梯度消失，而近距离梯度不会消失，从而导致总的梯度被近的梯度主导，同时总的梯度不会消失。</p>\n<p>这也是为什么RNN模型能以学到远距离依赖关系。</p>\n<p>简单的解释一下原因。</p>\n<p><strong>首先，我们要明白一点，RNN是共享一套参数的（输入参数，输出参数，隐层参数），这一点非常的重要</strong>。</p>\n<p>当然，我们在理解RNN的时候，会把RNN按照时间序列展开多个模块，可能会认为是多套参数，这个是不对的哈。</p>\n<p>如下所示：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-24-065733.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<p>然后，假设我们现在的时间序列为3，有如下公式存在：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-24-065732.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<p>现在假设我们只是使用t=3时刻的输出去训练模型，同时使用MSE作为损失函数，那么我们在t=3时刻，损失函数就是:</p>\n<p>$$L_{3}=\\frac{1}{2}(Y_{3}-O_{3})^{2}$$</p>\n<p>求偏导的时候，就是这样的情况：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-24-065731.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<p>其实看到这里，答案已经出来了。</p>\n<p>我们以第二个公式为例，也就是对$w_{x}$ 求偏导，如果时间序列程度为t，我们简化一下成下面这个公式：</p>\n<p>$$W_{x}=a_{1}+a_{2}+…+a_{t}$$</p>\n<p>时间序列越长，出现连乘的部分越集中出现在靠后面的公式上，比如$a_{t}$，但是前面的公式是不受影响的，比如$a_{1}$，也就是梯度是肯定存在的。</p>\n<p>总结一下：RNN中的梯度消失和普通网络梯度消失含义不同，它的真实含义是远距离的梯度消失，而近距离梯度不会消失，同时总的梯度不会消失，从而导致总的梯度被近的梯度主导。</p>\n","link":"links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/其他/RNN的梯度消失有什么与众不同的地方","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/其他/RNN的梯度消失有什么与众不同的地方/","reward":true}