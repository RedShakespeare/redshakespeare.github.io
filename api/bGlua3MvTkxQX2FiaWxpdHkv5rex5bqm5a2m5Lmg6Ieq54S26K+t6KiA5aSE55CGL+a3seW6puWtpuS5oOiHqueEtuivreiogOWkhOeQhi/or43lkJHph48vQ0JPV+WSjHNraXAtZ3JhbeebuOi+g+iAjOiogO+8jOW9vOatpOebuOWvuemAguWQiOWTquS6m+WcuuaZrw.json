{"title":"","date":"2025-12-02T22:37:33.083Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:37:33.083Z","content":"<p>CBOW和skip-gram相较而言，彼此相对适合哪些场景</p>\n<p>先用一句话来个结论：CBOW比Skip-gram 训练速度快，但是Skip-gram可以得到更好的词向量表达。</p>\n<p>为什么这么说？</p>\n<p>因为我们知道两种优化方式只是对softmax的近似优化，不会影响最终结果，所以这里，我们讨论的时候，为了更加的清晰讲解，不考虑优化的情况。</p>\n<p>使用一句话作为一个例子： “我/永远/爱/中国/共产党”</p>\n<p>先说CBOW，我们想一下，它的情况是使用周围词预测中心词。如果“爱”是中心词，别的是背景词。对于“爱”这个中心词，只是被预测了一次。</p>\n<p>对于Skip-gram，同样，我们的中心词是“爱”，背景词是其他词，对于每一个背景词，我们都需要进行一次预测，每进行一次预测，我们都会更新一次词向量。也就是说，相比CBOW，我们的词向量更新了2k次（假设K为窗口，那么窗口内包含中心词就有2k+1个单词）</p>\n<p>想一下是不是这么回事？Skip-gram被训练的次数更多，那么词向量的表达就会越丰富。</p>\n<p>如果语料库中，我们的的低频词很多，那么使用Skip-gram就会得到更好的低频词的词向量的表达，相应的训练时长就会更多。</p>\n<p>简单来说，我们视线回到一个大小为K的训练窗口（窗口内全部单词为2k+1个），CBOW只是训练一次，Skip-gram 则是训练了2K次。当然是Skip-gram词向量会更加的准确一点，相应的会训练的慢一点。</p>\n<p>欢迎大佬拍砖</p>\n","link":"links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/CBOW和skip-gram相较而言，彼此相对适合哪些场景","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/CBOW和skip-gram相较而言，彼此相对适合哪些场景/","reward":true}