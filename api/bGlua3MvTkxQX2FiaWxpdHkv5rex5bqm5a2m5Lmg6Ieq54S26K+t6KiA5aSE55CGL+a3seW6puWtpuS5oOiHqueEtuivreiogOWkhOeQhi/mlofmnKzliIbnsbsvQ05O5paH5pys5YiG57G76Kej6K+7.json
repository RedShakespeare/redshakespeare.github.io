{"title":"","date":"2025-12-02T22:25:48.693Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:25:48.693Z","content":"<p>TextCNN中的卷积核在文本进行处理的时候，是在文本长度上进行卷积。卷积核的大小<br>\n不同，带来的直接后果是这个卷积核每次滑动的时候处理的单词长度不同。<br>\n卷积核大小为2的时候，一次处理2-gram。卷积核大小为3-gram，一次处理三个大小的单词。<br>\n所以卷积核在对文本进行卷积的操作，更像是对在提取文本在n-gram上的特征。卷积核权重的更新<br>\n只是为了能够更好的提取n-gram上的特征。卷积核权重的更新<br>\n大小为2的卷积核提取的是2-gram特征，大小为3的卷积核提取的是3-gram特征，以此类推。<br>\n取不同卷积核大小进行卷积操作的原因，我的理解是可以提取这个句子多个维度不同的信息，使得特征更加的丰富。</p>\n<p>还有一点需要去注意的是，以2-gram为例，每次都是提取两个单词文本，但是如果文本很长，最后两个字和最开始的维度的单词<br>\n联系就很小，唯一的联系就是卷积核的权重是共享的。<br>\n举个例子：<br>\n今天天气不错，适合出去旅游</p>\n<p>在这句话中，如果卷积核大小为2，我们这里不考虑中文分词，那么今天 天天 两个词组中间出了有卷积核权重的联系还有天这个单词的共有性。<br>\n但是今天和旅游两个单词联系性在CNN中并没有体现出来。<br>\n这也就是为什么CNN不适合处理长文本的原因。</p>\n<p>卷积之后，接了一个最大池化。论文中给出的原因是因为输入句子长度不一定，经过卷积之后长度不一定，<br>\n如果直接操作的话，后面的全连接层权重形状不固定，不利于训练。<br>\n其实感觉这一点站不住脚，处理文本的时候，一般会固定长度，阶段长度，不存在卷积之后大小不一定的原因。</p>\n<p>但是如果我们在处理文本的时候，没有截断长度，而是排序然后按照batch中长读补长，是存在上述问题的，所以需要最大池化。</p>\n<p>上面这个原因感觉是最重要的，其实还有一个原因，论文中是说想要获取一个卷积核提取出来特征中的最重要的特征。我的可理解是<br>\n这个原因不太好，因为我直接用所有特征肯定比选取其中一个最重要的效果是好的。</p>\n<p>论文中把一个卷积核抽取特征，然后接一个最大池化的操作，形象的比喻为一个卷积核抽取一个特征。</p>\n<p>有一个人把特点总结的很到位，叫做CNN的卷积核实现了捕捉局部相关性</p>\n","link":"links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/CNN文本分类解读","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/CNN文本分类解读/","reward":true}