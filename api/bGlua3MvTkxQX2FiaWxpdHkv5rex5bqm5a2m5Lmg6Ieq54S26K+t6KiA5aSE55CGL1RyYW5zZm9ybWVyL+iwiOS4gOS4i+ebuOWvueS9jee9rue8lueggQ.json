{"title":"","date":"2025-12-02T22:16:21.928Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:16:21.928Z","content":"<p>谈一下相对位置编码RPR</p>\n<p>经过线性变化之后，正余弦函数表示的相对位置信息消失，所以需要优化。</p>\n<p>一般来讲，谈到优化，三种比较有名：RPR； Transformer-XL；complex embeddings；</p>\n<p>我在这个文章简单讲一下RPR。</p>\n<p>老样子，不涉及到公式推导，尽量把我的理解讲出来。</p>\n<h4 id=\"rpr思路\">RPR思路<a title=\"#rpr思路\" href=\"#rpr思路\"></a></h4>\n<p>RPR思路很简单，原始正余弦函数，是在输入的的时候与词向量相加作为输入，在attention丢失相对位置信息.</p>\n<p>改进的话就是不在输入的时候进行位置编码，而是在attention中显示把相对位置信息加入进去。</p>\n<h4 id=\"如何理解相对位置\">如何理解相对位置<a title=\"#如何理解相对位置\" href=\"#如何理解相对位置\"></a></h4>\n<p>绝对位置编码是在每个位置都对应一个唯一的位置编码信息，RPR把这一部分去掉去学习一个相对位置编码。</p>\n<p>首先我们需要知道相对位置是有方向的的。</p>\n<p>举个例子：”我/爱/中国/共产党“</p>\n<p>”我“对”爱“的相对位置就是 -1， ”中国“对”爱“的相对位置就是 1。</p>\n<p>所以方向不同，对应两个不同的相对位置，在学习的时候，一个距离，也就需要学习两个相对位置编码。</p>\n<h4 id=\"rpr修改思想\">RPR修改思想<a title=\"#rpr修改思想\" href=\"#rpr修改思想\"></a></h4>\n<p>作者认为在相对位置小于4的时候，attention对相对位置比较敏感，在大于4之后，相对位置不敏感。所以窗口设置为4。</p>\n<p>需要注意的是，窗口设为4，代表的当前位置左边4个，右边也有4个，再加上自己，就是一共9个位置，也就是：</p>\n<p>$[i-4,i-3,i-2,i-1,i,i+1,i+2,i+3,i+4]$</p>\n<p>注解：有方向</p>\n<p>当你的attention进行到哪个单词的时候，你的 $i$ 就对应的是哪个位置。</p>\n<p>还是上面那句话举例子。</p>\n<p>如果此时的输入是“我”，那么用到的相对位置编码就是$[i,i+1,i+2,i+3]$</p>\n<p>如果此时输入的是“爱”，那么这个时候用到的相对位置编码就是$[i-1,i,i+1,i+2]$</p>\n<p>了解了这个，我们再谈一下这个相对位置信息是怎么显示加入进去的。</p>\n<p>这个显示的加入分为两个部分。</p>\n<p>第一个部分是在计算$e_{ij}$的时候，涉及到RPR的一个表征:$a_{ij}^{K}$，表示对 Q/K/V三者中的K做了修改。</p>\n<p>第二个部分就是在计算$z_{i}$的时候，涉及到另一个RPR的表征:$a_{ij}^{V}$，表示对Q/K/V三者中的V做了修改。</p>\n<p>两个部分的修改都是使用加法。</p>\n<p>关于RPR大概就讲这么多吧。其实思路还是比较简单的，总结来说，就是把相对位置信息在attention之中，显势的加入进去，而不是在输入的时候与词向量相加。</p>\n<p>如果觉得对您有点帮助，点个赞再走吧。</p>\n<p>参考资料</p>\n<p><a href=\"https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E5%8E%9F%E7%89%88Transformer%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%A9%B6%E7%AB%9F%E6%9C%89%E6%B2%A1%E6%9C%89%E5%8C%85%E5%90%AB%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF.md\" target=\"_blank\">dasou</a></p>\n","link":"links/NLP_ability/深度学习自然语言处理/Transformer/谈一下相对位置编码","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/谈一下相对位置编码/","reward":true}