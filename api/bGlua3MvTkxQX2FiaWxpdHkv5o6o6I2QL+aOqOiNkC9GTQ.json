{"title":"","date":"2025-12-02T22:37:32.904Z","date_formatted":{"ll":"Dec 2, 2025","L":"12/02/2025","MM-DD":"12-02"},"updated":"2025-12-02T14:37:32.904Z","content":"<h4 id=\"两个核心细节\">两个核心细节<a title=\"#两个核心细节\" href=\"#两个核心细节\"></a></h4>\n<p>掌握FM，有两个细节需要注意：参数量级的变化和时间复杂度的变化。</p>\n<p>首先对于参数量级，由线性模型到多项式模型到FM模型参数量级变化为：</p>\n<p>n–&gt;n*n–&gt;kn (k&lt;&lt;n)</p>\n<p>其次是由原始FM公式到化简之后的FM公式复杂度的变化情况为：</p>\n<p>Kn*n–&gt;kn</p>\n<h4 id=\"线性模型\">线性模型<a title=\"#线性模型\" href=\"#线性模型\"></a></h4>\n<p>回归问题我们一般使用的比较见得baseline就是线性回归，二元分类问题就是逻辑回归LR。</p>\n<p>线性模型公式如下（回归问题）：</p>\n<p><img src=\"/links/NLP_ability/%E6%8E%A8%E8%8D%90/images/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.png\" alt=\"线性模型\" loading=\"lazy\" class=\"φbp\"></p>\n<p>对于线性模型，我们的假设一般是认为特征之间是相互独立的，无法学习到特征之间的交叉影响。</p>\n<p>为了解决特征交叉的问题，我们一般可以人为的加入一些自己的先验信息，比如做一些特征之间的交互，不过这个很需要人们的经验。</p>\n<h4 id=\"poly2模型--暴力组合特征交叉\">POLY2模型–暴力组合特征交叉<a title=\"#poly2模型--暴力组合特征交叉\" href=\"#poly2模型--暴力组合特征交叉\"></a></h4>\n<p>这个时候，POLY2模型成了可行的方案。POLY2 模型，对所有特征做了两两交叉，并对所有特征组合赋予了权重，在一定程度上解决了特征组合问题，本质仍然是线性模型，训练方法与逻辑回归没有区别。</p>\n<p>我们把POLY2（只是特征两两交叉的部分）加到线性模型中，从而模型可以过渡到多项式模型，公式如下：</p>\n<p><img src=\"/links/NLP_ability/%E6%8E%A8%E8%8D%90/images/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B.png\" alt=\"多项式模型\" loading=\"lazy\" class=\"φbp\"></p>\n<p>（ps：看到这里我自己有一个疑问，同一个特征onehot之后，会在自己里面做特征交叉吗）</p>\n<p>看这个公式，主要是看后面那个交叉的部分。看到这部分，其实很容联想到我们在LR中自己加入交叉特征的部分。</p>\n<p>但是需要注意的是，这里有点像暴力求解一样，忽视或者说省去了人工先验的部分，直接做到了所有特征之间的交叉，然后去求解对应的参数就可以。</p>\n<h5 id=\"poly2模型两个问题\">POLY2模型两个问题<a title=\"#poly2模型两个问题\" href=\"#poly2模型两个问题\"></a></h5>\n<p>但是这样暴力求解存在两个问题：参数量和参数稀疏导致学习困难的问题。</p>\n<p>先说参数量的问题，如果我自身特征（未交叉之前）就已经很多了，为n，那么交叉之后就是一个 n*n级别的参数量。极端情况会出现参数的量级比样本量级都大，训练起来及其的困难。</p>\n<p>再说参数稀疏的问题。互联网数据通常使用one-hot编码除了类别型数据，从而使特征向量极度稀疏，POLY2模型做无选择的特征交叉，使得特征向量更加的稀疏，导致大部分交叉特征的权重缺乏有效的数据进行训练，无法收敛。</p>\n<p>我自己理解的时候感觉这个很像是NLP中OOV情况。</p>\n<h4 id=\"fm模型\">FM模型<a title=\"#fm模型\" href=\"#fm模型\"></a></h4>\n<p>面对这两种问题，FM模型怎么解决呢？</p>\n<p>FM相比于POLY2模型，主要区别是用两个向量的内积代替了单一权重系数，具体来说，就是FM为每个特征学习了一个隐权重向量。在特征交叉的时候，使用两个特征向量的内积作为交叉特征的权重。</p>\n<p>这样其实就解决了上面两个问题。</p>\n<p>参数量的问题变为了 kn个参数，因为每个特征对应一个K维度的向量。</p>\n<p>其次是参数学习的问题。OOV问题很大缓解，即使当前特征交叉在训练样本中没出现过，但是每个特征已经学到了自己embedding，内积之后是有结果的。这也是为什么FM模型泛化能力强的根本原因。</p>\n<p>FM模型如下：</p>\n<p><img src=\"/links/NLP_ability/%E6%8E%A8%E8%8D%90/images/FM%E6%A8%A1%E5%9E%8B.png\" alt=\"FM模型\" loading=\"lazy\" class=\"φbp\"></p>\n<p>其中涉及到的二阶部分可以通过公式的化简从Kn*n–&gt;kn：</p>\n<p><img src=\"/links/NLP_ability/%E6%8E%A8%E8%8D%90/images/FM%E4%BA%8C%E9%98%B6%E5%85%AC%E5%BC%8F%E5%8C%96%E7%AE%80.png\" alt=\"FM二阶公式化简\" loading=\"lazy\" class=\"φbp\"></p>\n<p>参考链接：</p>\n<p>文章：</p>\n<p>FM算法解析 - 王多鱼的文章 - 知乎 <a href=\"https://zhuanlan.zhihu.com/p/37963267\" target=\"_blank\">https://zhuanlan.zhihu.com/p/37963267</a></p>\n<p>推荐系统召回四模型之：全能的FM模型 - 张俊林的文章 - 知乎 <a href=\"https://zhuanlan.zhihu.com/p/58160982\" target=\"_blank\">https://zhuanlan.zhihu.com/p/58160982</a></p>\n<p>代码：</p>\n<p>deepctr-torch 大概跑了一遍</p>\n","link":"links/NLP_ability/推荐/推荐/FM","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/推荐/推荐/FM/","reward":true}